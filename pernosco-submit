#!/usr/bin/env python3

import base64
import hashlib
import json
import os
import random
import re
import shutil
import subprocess
import sys
import tempfile
import time
import zipfile

class CustomException(Exception):
    pass

def usage():
    print("usage: %s upload <trace-dir> [<source-path>...]"%sys.argv[0], file=sys.stderr)
    print("       %s keygen <provided-secret>"%sys.argv[0], file=sys.stderr)
    print("Only source files under the given path(s) will be uploaded.", file=sys.stderr)
    sys.exit(1)

def strip_wrapper(s):
    ret = ""
    for line in s.splitlines():
        if line.startswith("----"):
            continue
        ret += line.strip()
    return ret

def check_executable(executable, package):
    if not shutil.which(executable):
        print("Cannot find `%s`. Please install package `%s`."%(executable, package), file=sys.stderr)
        sys.exit(1)

def keygen():
    if len(sys.argv) != 3:
        usage()
    check_executable('openssl', 'openssl')

    provided = sys.argv[2]
    if len(provided.split(',')) != 2:
        print("Invalid <provided-secret> argument", file=sys.stderr)
    full_key = subprocess.check_output(['openssl', 'ecparam', '-genkey', '-name', 'prime256v1', '-noout'])
    public_key = subprocess.check_output(['openssl', 'ec', '-pubout'], input=full_key, stderr=subprocess.DEVNULL).decode('utf-8')
    print("Private key: PERNOSCO_USER_SECRET_KEY=%s,%s"%(provided, strip_wrapper(full_key.decode('utf-8'))))
    print("Public key: %s"%strip_wrapper(public_key))
    sys.exit(0)

if len(sys.argv) < 2:
    usage()
cmd = sys.argv[1]
if cmd == 'keygen':
    keygen()
if cmd != 'upload' or len(sys.argv) < 3:
    usage()

check_executable('openssl', 'openssl')
check_executable('tar', 'tar')
check_executable('zstdmt', 'zstd')
if not shutil.which('aws'):
    print("Please install the AWS command-line tools using", file=sys.stderr)
    print("  sudo pip3 install awscli --upgrade", file=sys.stderr)
    print("(Distribution packages may fail due to https://github.com/aws/aws-cli/issues/2403.)", file=sys.stderr)
    sys.exit(1)
if not shutil.which('rr'):
    print("Please install `rr` master and make sure it's on your $PATH.", file=sys.stderr)
    sys.exit(1)
status = subprocess.run(['rr', 'sources', '/dev/null'], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)
if status.returncode != 65:
    print("rr is out of date. Please install `rr` master and make sure it's on your $PATH.", file=sys.stderr)
    sys.exit(1)

trace_dir = sys.argv[2]
source_dirs = []
for path in sys.argv[3:]:
    # Apply realpath since our processing below depends on that
    source_dirs.append(os.path.realpath(path))
if not os.path.isfile("%s/version"%trace_dir):
    print("Can't find rr trace in %s"%trace_dir, file=sys.stderr)
    sys.exit(1)

if not 'PERNOSCO_USER' in os.environ:
    print("PERNOSCO_USER not set", file=sys.stderr)
    sys.exit(1)
pernosco_user = os.environ['PERNOSCO_USER']
if not 'PERNOSCO_GROUP' in os.environ:
    print("PERNOSCO_GROUP not set", file=sys.stderr)
    sys.exit(1)
pernosco_group = os.environ['PERNOSCO_GROUP']
if not 'PERNOSCO_USER_SECRET_KEY' in os.environ:
    print("PERNOSCO_USER_SECRET_KEY not set", file=sys.stderr)
    sys.exit(1)
pernosco_user_secret_key = os.environ['PERNOSCO_USER_SECRET_KEY']

os.makedirs("%s/files.user"%trace_dir, exist_ok=True)

def write_metadata():
    with open('%s/files.user/user'%trace_dir, "wt", encoding='utf-8') as f:
        print(pernosco_user, file=f)
    with open('%s/files.user/group'%trace_dir, "wt", encoding='utf-8') as f:
        print(pernosco_group, file=f)

def rr_pack():
    print("Running 'rr pack'...")
    subprocess.check_call(['rr', 'pack', trace_dir])

def package_libthread_db():
    for f in ['/usr/lib64/libthread_db.so',
              '/usr/lib/x86_64-linux-gnu/libthread_db.so']:
        if os.path.isfile(f):
            os.makedirs('%s/files.system-debuginfo'%trace_dir, exist_ok=True)
            print("Copying %s into trace..."%f)
            shutil.copyfile(f, '%s/files.system-debuginfo/libthread_db.so'%trace_dir)
            break

github_re = re.compile('(https://github.com/|git@github.com:)([^/]+)/(.*)')
gitlab_re = re.compile('(https://gitlab.com/|git@gitlab.com:)([^/]+)/(.*)')
googlesource_re = re.compile('https://([^.]+.googlesource.com)/(.*)')

def strip(s, m):
    if s.endswith(m):
        return s[:(len(s) - len(m))]
    return s

def git_remote_url_to_source_url_generator(remote_url):
    m = github_re.match(remote_url)
    if m:
        return lambda rev: ("https://raw.githubusercontent.com/%s/%s/%s/"%(m.group(2), strip(m.group(3), ".git"), rev), None)
    m = gitlab_re.match(remote_url)
    if m:
        return lambda rev: ("https://gitlab.com/%s/%s/raw/%s/"%(m.group(2), strip(m.group(3), ".git"), rev), None)
    m = googlesource_re.match(remote_url)
    if m:
        # googlesource uses gitiles
        return lambda rev: ("https://%s/%s/+/%s/"%(m.group(1), m.group(2), rev), "?format=TEXT")
    return None

# Returns a hash of remote names to generators of URLs Pernosco can use to fetch files
def git_remotes(repo_path):
    output = subprocess.check_output(['git', 'remote', '--verbose'], cwd=repo_path).decode('utf-8')
    ret = {}
    for line in output.splitlines():
        [remote, url, token] = line.split()[:3]
        if token != "(fetch)":
            continue
        url_generator = git_remote_url_to_source_url_generator(url)
        if url_generator != None:
            ret[remote] = url_generator
    return ret

def git_find_rev(repo_path, remotes):
    git = subprocess.Popen(['git', 'log', '--format=%H %D'],
                           cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    for line in git.stdout:
        line = line.decode('utf-8')
        revision = line.split()[0]
        for token in line.split():
            if "/" in token:
                remote = token.split('/')[0]
                if remote in remotes:
                    git.kill()
                    git.wait()
                    return (revision, remote)
    git.wait()
    return None

def git_committed_files(repo_path, revision, files):
    h = {}
    for f in files:
        h[f] = True
    git = subprocess.Popen(['git', 'diff', '--name-only', revision, 'HEAD'],
                           cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret = {}
    for line in git.stdout:
        file = line.decode('utf-8').rstrip()
        if file in h:
            ret[file] = True
    git.wait()
    return ret

# Computes the files under repo_path that aren't fully committed to HEAD
# (i.e. ignored, untracked, modified in the working area, modified in the git index).
# returns the result as a hash-set.
def git_changed_files(repo_path, files):
    h = {}
    for f in files:
        h[f] = True
    git = subprocess.Popen(['git', 'status', '--untracked-files=all', '--ignored', '--short'],
                           cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret = {}
    for line in git.stdout:
        line = line.decode('utf-8')
        if line[2] != ' ':
            raise CustomException("Unexpected line: %s"%line)
        file = line[3:].rstrip()
        if file in h:
            ret[file] = True
    git.wait()
    return ret

def analyze_git_repo(repo_path, files):
    remotes = git_remotes(repo_path)
    if len(remotes) == 0:
        print("No remotes found for Git repo %s, packaging files instead..."%repo_path)
        return (None, files)
    r = git_find_rev(repo_path, remotes)
    if not r:
        print("Can't find usable remote master for Git repo %s, packaging files instead..."%repo_path)
        return (None, files)
    (revision, remote) = r
    (url, url_suffix) = remotes[remote](revision)
    print("Git repo at %s: Checking for source files changed since revision %s in remote %s (access via %s)..."%(repo_path, revision, remote, url), end="")
    # Collect files changed between `revision` and HEAD
    out_files = git_committed_files(repo_path, revision, files)
    # Collect files changed between HEAD and working dir
    changed_files = git_changed_files(repo_path, files)
    out_files_len = len(out_files)
    out_files.update(changed_files)
    if len(out_files) == 0:
        print(" no changes")
    else:
        print(" %d files with committed changes, %d files changed since HEAD, %d overall"%(out_files_len, len(changed_files), len(out_files)))
    mount = {'url': url, 'at': repo_path}
    if url_suffix:
         mount['urlSuffix'] = url_suffix
    return (mount, out_files)

mozilla_re = re.compile('(https|ssh)://hg.mozilla.org/(.*)')

def hg_remote_url_to_source_url_generator(remote_url):
    m = mozilla_re.match(remote_url)
    if m:
        if m.group(2) == 'try':
            # Ignore 'try' because it gets purged frequently
            return None
        return lambda rev: ("https://hg.mozilla.org/%s/raw-file/%s/"%(m.group(2), rev), None)
    return None

# Returns a hash of remote names to generators of URLs Pernosco can use to fetch files
def hg_remotes(repo_path):
    output = subprocess.check_output(['hg', 'paths'], cwd=repo_path).decode('utf-8')
    ret = {}
    for line in output.splitlines():
        [remote, equals, url] = line.split()[:3]
        url_generator = hg_remote_url_to_source_url_generator(url)
        if url_generator != None:
            ret[remote] = url_generator
    return ret

def hg_find_rev(repo_path, remotes):
    best_rev_num = 0
    best_sha = None
    best_remote = None
    for r in remotes:
        output = subprocess.check_output(['hg', 'log', '-T', '{rev} {node}', '-r', "ancestor((parents(outgoing('%s') & ancestors(.))) | .)"%r],
                                         cwd=repo_path).decode('utf-8')
        [rev_num, sha] = output.split()[:2]
        rev_num = int(rev_num)
        if rev_num > best_rev_num:
            best_rev_num = rev_num
            best_sha = sha
            best_remote = r
    if best_rev_num == 0:
        return None
    return (best_sha, best_remote)

def hg_changed_files(repo_path, revision, files):
    h = {}
    for f in files:
        h[f] = True
    hg = subprocess.Popen(['hg', 'status', '-nmaui', '--rev', revision],
                           cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret = {}
    for line in hg.stdout:
        file = line.decode('utf-8').rstrip()
        if file in h:
            ret[file] = True
    hg.wait()
    return ret

def analyze_hg_repo(repo_path, files):
    remotes = hg_remotes(repo_path)
    if len(remotes) == 0:
        print("No remotes found for Mercurial repo %s, packaging files instead..."%repo_path)
        return (None, files)
    r = hg_find_rev(repo_path, remotes)
    if not r:
        print("Can't find usable remote master for Mercurial repo %s, packaging files instead..."%repo_path)
        return (None, files)
    (revision, remote) = r
    (url, url_suffix) = remotes[remote](revision)
    print("Mercurial repo at %s: Checking for source files changed since revision %s in remote '%s' (access via %s)..."%(repo_path, revision, remote, url), end="")
    # Collect files changed between `revision` and HEAD
    out_files = hg_changed_files(repo_path, revision, files)
    if len(out_files) == 0:
        print(" no changes")
    else:
        print(" %d files changed"%len(out_files))
    mount = {'url': url, 'at': repo_path}
    if url_suffix:
         mount['urlSuffix'] = url_suffix
    return (mount, out_files)

def analyze_repo(repo_path, files):
    if os.path.isdir(os.path.join(repo_path, ".git")):
        return analyze_git_repo(repo_path, files)
    if os.path.isdir(os.path.join(repo_path, ".hg")):
        return analyze_hg_repo(repo_path, files)
    return (None, files)

disallowed_file_count = 0;

def allowed_file(file):
    global disallowed_file_count
    for f in source_dirs:
        if file.startswith(f):
            return True
    disallowed_file_count += 1
    if disallowed_file_count <= 10:
        print("Not uploading source file %s (add an allowed source directory to the command line?)"%file)
    if disallowed_file_count == 11:
        print("(too many disallowed-source-file warnings, suppressing the rest)")
    return False

def package_source_files():
    print("Obtaining source file list...")
    output = subprocess.check_output(['rr', 'sources', trace_dir]).decode('utf-8')
    rr_sources = json.loads(output)
    out_sources = {};
    or_condition = [];
    for b in rr_sources['relevant_binaries']:
        or_condition.append({'binary':b})
    out_sources['condition'] = {'or': or_condition}
    explicit_files = []
    out_mounts = []
    repo_paths = []
    non_repo_files_count = 0;
    # Mount repos
    for repo_path in rr_sources['files']:
        files = rr_sources['files'][repo_path]
        if repo_path == '':
            non_repo_files_count = len(files)
            explicit_files.extend(files)
            continue
        repo_paths.append(repo_path)
        (repo_mount, modified_files) = analyze_repo(repo_path, files)
        for m in modified_files:
            explicit_files.append(os.path.join(repo_path, m))
        if repo_mount == None:
            continue
        out_mounts.append(repo_mount)
    # Install non-repo files
    print("Packaging %d modified and %d non-repository files..."%(len(explicit_files) - non_repo_files_count, non_repo_files_count))
    with zipfile.ZipFile('%s/files.user/sources.zip'%trace_dir, mode='w', compression=zipfile.ZIP_DEFLATED) as zip_file:
        for f in explicit_files:
            if allowed_file(f):
                zip_file.write(f)
            else:
                content = ("/* This file was not uploaded because the path %s is not under the allowed directories [%s] */"%
                    (f, ", ".join(['"%s"'%d for d in source_dirs])))
                zip_file.writestr(f, content)
    out_mounts.append({'archive': 'files.user/sources.zip', 'at': '/'})
    # Add symlinks
    for s in rr_sources['symlinks']:
        out_mounts.append({'at': s['to'], 'link': s['from']})
    # Dump output
    out_sources['files'] = out_mounts
    out_sources['relevance'] = 'Relevant'
    with open('%s/sources.user'%trace_dir, "wt") as f:
        json.dump([out_sources], f, indent=2)
    return repo_paths

# The files under these paths are copied into gdbinit/
gdb_paths = [
    # Mozilla
    '.gdbinit',
    '.gdbinit_python',
    'third_party/python/gdbpp',
    # Chromium
    'tools/gdb',
    'third_party/libcxx-pretty-printers',
    'third_party/blink/tools/gdb',
]

def package_gdbinit(repo_paths):
    gdbinit_dir = "%s/gdbinit"%trace_dir
    shutil.rmtree(gdbinit_dir, ignore_errors=True)
    gdbinit_sub_paths = []
    for repo in repo_paths:
        sub_path = repo.replace("/", "_");
        for g in gdb_paths:
            path = os.path.join(repo, g)
            out_path = "%s/%s/%s"%(gdbinit_dir, sub_path, g)
            if os.path.isfile(path):
                os.makedirs(os.path.dirname(out_path), exist_ok=True)
                print("Copying file %s into trace"%path)
                shutil.copy(path, out_path)
            elif os.path.isdir(path):
                os.makedirs(os.path.dirname(out_path), exist_ok=True)
                print("Copying tree %s into trace"%path)
                shutil.copytree(path, out_path, copy_function=shutil.copy)
        # Install our own Pernosco-compatible .gdbinit for Chromium
        if os.path.isfile("%s/%s/tools/gdb/gdb_chrome.py"%(gdbinit_dir, sub_path)):
            with open("%s/%s/.gdbinit"%(gdbinit_dir, sub_path), "wt") as f:
                f.write("""python
import sys
sys.path.insert(0, "/trace/gdbinit/%s/tools/gdb/")
sys.path.insert(0, "/trace/gdbinit/%s/third-party/libcxx-pretty-printers/")
import gdb_chrome
from libcxx.v1.printers import register_libcxx_printers
register_libcxx_printers(None)
gdb.execute('source /trace/gdbinit/%s/tools/gdb/viewg.gdb')
end
"""%(sub_path, sub_path, sub_path))
        if os.path.isfile("%s/%s/.gdbinit"%(gdbinit_dir, sub_path)):
            gdbinit_sub_paths.append(sub_path)
    if len(gdbinit_sub_paths) > 0:
        with open("%s/.gdbinit"%gdbinit_dir, "wt") as f:
            for sub_path in gdbinit_sub_paths:
                print("directory /trace/gdbinit/%s"%sub_path, file=f)
                print("source /trace/gdbinit/%s/.gdbinit"%sub_path, file=f)

def upload():
    [aws_access_key_id, aws_secret_access_key, pernosco_secret_key] = pernosco_user_secret_key.split(',')
    with tempfile.NamedTemporaryFile(mode="w+b") as payload_file:
        print("Compressing to %s..."%payload_file.name)
        with tempfile.TemporaryFile(mode="w+t", encoding='utf-8') as key_file:
            print("-----BEGIN EC PRIVATE KEY-----\n%s\n-----END EC PRIVATE KEY-----"%pernosco_secret_key, file=key_file)
            key_file.seek(0)
            public_key = subprocess.check_output(['openssl', 'ec', '-pubout'], stdin=key_file, stderr=subprocess.DEVNULL).decode('utf-8')
            p0 = subprocess.Popen(["tar", "-I", "zstdmt", "--exclude", "db*", "-cf", "-", "."], cwd=trace_dir, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
            p1 = subprocess.Popen(["tee", payload_file.name], stdin=p0.stdout, stdout=subprocess.PIPE)
            p0.stdout.close()
            os.set_inheritable(key_file.fileno(), True)
            p2 = subprocess.Popen(["openssl", "dgst", "-sha256", "-sign", "/proc/self/fd/%d"%key_file.fileno()], close_fds=False, stdin=p1.stdout, stdout=subprocess.PIPE)
            p1.stdout.close()
            (sig, err) = p2.communicate()
        if err:
            raise CustomException("openssl failed: %s"%err)
        p0.wait()
        p1.wait()
        if p0.returncode != 0:
            raise CustomException("tar failed: %d"%p0.returncode)
        if p1.returncode != 0:
            raise CustomException("tee failed: %d"%p1.returncode)
        signature = base64.urlsafe_b64encode(sig).decode('utf-8')
        # Create a nonce that's 64 bits of the SHA256 of the signature.
        hasher = hashlib.sha256()
        hasher.update(sig)
        # Strip '=' because it requires %-encoding in URLs
        nonce = base64.urlsafe_b64encode(hasher.digest()[:8]).decode('utf-8').rstrip('=')
        s3_url = "s3://pernosco-upload/%s.tar.zst"%nonce
        print("Uploading %d bytes to %s..."%(os.path.getsize(payload_file.name), s3_url))
        s3_env = dict(os.environ,
            AWS_DEFAULT_REGION='us-east-2',
            AWS_ACCESS_KEY_ID=aws_access_key_id,
            AWS_SECRET_ACCESS_KEY=aws_secret_access_key)
        # Send the public key with the signature so the server can easily
        # determine which key was used and check that the key is authorized
        metadata = "publickey=%s,signature=%s,user=%s,group=%s"%(strip_wrapper(public_key), signature, pernosco_user, pernosco_group)
        if 'PERNOSCO_EXTRA_METADATA' in os.environ:
            metadata += ",%s"%os.environ['PERNOSCO_EXTRA_METADATA']
        subprocess.check_call(["aws", "s3", "cp", "--metadata", metadata, payload_file.name, s3_url], env=s3_env)

write_metadata()
rr_pack()
package_libthread_db()
repo_paths = package_source_files()
package_gdbinit(repo_paths)
upload()
